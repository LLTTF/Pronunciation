# Pronunciation

**Project Overview**
This project aims to develop a system that evaluates the fluency and pronunciation quality of spoken audio. The system uses a combination of machine learning models and feature extraction techniques to assess pronunciation fluency, provide feedback, and recommend corrections. The process involves multiple stages, including audio feature extraction, fluency scoring using a neural network, and transcription comparison using an automatic speech recognition (ASR) model.

**Key Features**
Pronunciation Fluency Scoring: The system evaluates the fluency of spoken audio and categorizes it into three levels: Low, Intermediate, and High.

Phoneme Matching: Compares the predicted phonemes with the expected phonemes to assess pronunciation accuracy.

ASR Transcription Feedback: Uses the Wav2Vec2 ASR model to transcribe the audio and compare it with the expected transcription, providing feedback.

Multi-Model Integration: The system integrates an LSTM-based fluency scoring model and a pre-trained ASR model (Wav2Vec2) to provide comprehensive feedback on pronunciation.

**Project Structure**
The system follows a pipeline that processes audio input and evaluates its pronunciation quality:

Data Preprocessing and Feature Extraction

The audio input is processed to extract relevant features that help assess pronunciation quality.

Features like MFCCs (Mel Frequency Cepstral Coefficients), RMSE (Root Mean Square Error), Spectral Flux, and Zero Crossing Rate are computed using the librosa library.

These features are then fed into the fluency model to predict the fluency level.

**Training the Fluency Scoring Model**

A Recurrent Neural Network (RNN) with LSTM layers is built using Keras to predict pronunciation fluency.

The dataset (feat.npy for features and label.npy for labels) is split into training and testing sets using train_test_split from sklearn.

The model is trained with categorical cross-entropy loss and the RMSProp optimizer.

**Fluency Prediction**

The trained model predicts the fluency level of the pronunciation for a given audio clip.

The predicted fluency level is categorized into one of three classes: Low, Intermediate, or High.

If the fluency level is Low or Intermediate, the system performs further analysis by comparing the transcribed audio with the expected transcription.

**ASR Transcription and Feedback**

The Wav2Vec2 model is used to transcribe the audio and generate the predicted transcription.

The predicted transcription is compared to the expected transcription. If a discrepancy is found, the system provides suggestions for correcting the pronunciation.

**Technical Details**
1. Data Preparation
Input Audio: The dataset consists of 1400 audio files containing speech samples.

Feature Extraction: Features are extracted from the raw audio using librosa, including MFCCs, RMSE, Spectral Flux, and Zero Crossing Rate.

2. Model Architecture
Fluency Scoring Model:

The fluency model is a LSTM-based Recurrent Neural Network built using Keras. The model has the following layers:

Two LSTM layers (256 and 32 units respectively).

A Dense layer with a softmax activation function for classifying the fluency level.

The model classifies the fluency of the audio clip into three categories: Low, Intermediate, or High.

ASR Model:

The Wav2Vec2 model from Hugging Face is used for transcription. It is a pre-trained automatic speech recognition (ASR) model fine-tuned on large datasets, capable of transcribing spoken audio into text.

3. Training Details
The fluency scoring model is trained with a batch size of 64 and 60 epochs.

The loss function is categorical cross-entropy, suitable for multi-class classification tasks.

The optimizer used is RMSProp, which is effective for training RNNs.

4. Feature Extraction
MFCCs (Mel Frequency Cepstral Coefficients): These coefficients represent the short-term power spectrum of the audio.

RMSE (Root Mean Square Error): This feature measures the loudness of the audio signal.

Spectral Flux: This feature captures changes in the spectral content of the audio.

Zero Crossing Rate: This feature counts the number of times the signal changes polarity, which can indicate the signalâ€™s frequency characteristics.

5. Fluency Scoring and Feedback
The fluency level is predicted based on the extracted features using the trained model.

If the fluency score is Low or Intermediate, further feedback is generated by transcribing the audio using the ASR model. The transcription is compared to the expected transcription, and if discrepancies are found, pronunciation correction suggestions are provided.

**How to Use**
Set up your environment:

Clone the repository.

Install the required dependencies 

Upload your audio file:

Place the audio file (e.g., .wav, .m4a, .mp3) in the /input directory.

Run the fluency evaluation:

Run the script to load the audio file, extract features, predict the fluency level, and provide feedback.

Interpret the output:

The system will output the predicted fluency level (Low, Intermediate, or High).

If the fluency is not "High", the ASR transcription and a correction suggestion will be provided.

**Future Enhancements**
Accent-Specific Models: Fine-tuning the fluency model for different speech accents could improve feedback accuracy.

Real-time Feedback: Implementing real-time voice input and feedback generation would enhance interactivity.

Advanced Phoneme Matching: More precise phoneme-level comparison methods can be incorporated for detailed pronunciation analysis.
